{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26253,"status":"ok","timestamp":1728034656017,"user":{"displayName":"Eiliya Zanganeh","userId":"16195051509224005628"},"user_tz":-210},"id":"FHgXc1f6P68R","outputId":"90008216-c9d3-4da7-bad5-eb8380ca2e68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["project_path = '/content/drive/MyDrive/NLP Projects/Sentiment analysis'\n","data_batch_count = 20000"],"metadata":{"id":"Z0sLTYKRGj4T","executionInfo":{"status":"ok","timestamp":1728034658564,"user_tz":-210,"elapsed":407,"user":{"displayName":"Eiliya Zanganeh","userId":"16195051509224005628"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"x9xuXPFNQAId","executionInfo":{"status":"ok","timestamp":1728035710422,"user_tz":-210,"elapsed":10850,"user":{"displayName":"Eiliya Zanganeh","userId":"16195051509224005628"}}},"outputs":[],"source":["import pandas as pd\n","\n","column_names = ['score', 'title', 'text']\n","train_dataset = pd.read_csv(f'{project_path}/Dataset/train.csv', names=column_names, nrows=1000000)\n","test_dataset = pd.read_csv(f'{project_path}/Dataset/test.csv', names=column_names, nrows=100000)"]},{"cell_type":"code","source":["train_dataset['score'] = train_dataset['score'] - 1\n","test_dataset['score'] = test_dataset['score'] - 1"],"metadata":{"id":"r6khkdvwWrJg","executionInfo":{"status":"ok","timestamp":1728035730088,"user_tz":-210,"elapsed":389,"user":{"displayName":"Eiliya Zanganeh","userId":"16195051509224005628"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["print(train_dataset.iloc[0]['score'])\n","print(train_dataset.iloc[0]['title'])\n","print(train_dataset.iloc[0]['text'])\n","\n","print(train_dataset['score'].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iWk6Et0mWLiW","executionInfo":{"status":"ok","timestamp":1728035730469,"user_tz":-210,"elapsed":2,"user":{"displayName":"Eiliya Zanganeh","userId":"16195051509224005628"}},"outputId":"ab36598e-b38c-4aa6-b5f4-36e0796aeb4a"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","Stuning even for the non-gamer\n","This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n","score\n","1    505678\n","0    494322\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","test_texts, test_labels = shuffle(test_dataset['text'], test_dataset['score'], random_state=42)\n","\n","train_texts, val_texts, train_labels, val_labels = train_test_split(\n","    train_dataset['text'], train_dataset['score'], test_size=0.05, shuffle=True, random_state=42\n",")"],"metadata":{"id":"Mp68MovVU4nE","executionInfo":{"status":"ok","timestamp":1728035732331,"user_tz":-210,"elapsed":446,"user":{"displayName":"Eiliya Zanganeh","userId":"16195051509224005628"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":381,"status":"ok","timestamp":1728035734573,"user":{"displayName":"Eiliya Zanganeh","userId":"16195051509224005628"},"user_tz":-210},"id":"BflXJV8MQi2x","outputId":"08875aac-aff4-492f-9144-70b84c89690d"},"outputs":[{"output_type":"stream","name":"stdout","text":["train dataset count 950000 - 950000\n","validation dataset count 50000 - 50000\n","test dataset count 100000 - 100000\n"]}],"source":["print(f\"train dataset count {len(train_texts)} - {len(train_labels)}\")\n","print(f\"validation dataset count {len(val_texts)} - {len(val_labels)}\")\n","print(f\"test dataset count {len(test_texts)} - {len(test_labels)}\")"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wj0MBWQiVYo2","executionInfo":{"status":"ok","timestamp":1728035846281,"user_tz":-210,"elapsed":98675,"user":{"displayName":"Eiliya Zanganeh","userId":"16195051509224005628"}},"outputId":"723b3668-3be1-4707-8cf7-385baf71948c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Text cleaning completed for both datasets.\n"]}],"source":["# Import required libraries\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt')\n","\n","def clean_text(text):\n","    # Convert text to lowercase\n","    text = text.lower()\n","\n","    # Remove numbers and special characters\n","    text = re.sub(r'\\d+', '', text)  # Remove numbers\n","    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n","    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n","\n","    return text\n","\n","# Apply text cleaning\n","train_texts = train_texts.apply(clean_text)\n","val_texts = val_texts.apply(clean_text)\n","test_texts = test_texts.apply(clean_text)\n","\n","print(\"Text cleaning completed for both datasets.\")"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"1aAaS8VOiKL3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"766db02e-df7b-450a-ace5-74454ac7decc","executionInfo":{"status":"ok","timestamp":1728038575434,"user_tz":-210,"elapsed":2700773,"user":{"displayName":"Eiliya Zanganeh","userId":"16195051509224005628"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Start processing train data\n","Finished processing train data\n","Start processing validation data\n","Finished processing validation data\n","Start processing test data\n","Finished processing test data\n"]}],"source":["import torch\n","from transformers import BertTokenizer\n","\n","# Load pre-trained BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenization and saving function with labels\n","def batch_tokenize_and_save_with_labels(texts, labels, file_prefix, batch_size=data_batch_count):\n","    print(f'Start processing {file_prefix} data')\n","    for i in range(0, len(texts), batch_size):\n","        batch_texts = texts[i:i+batch_size]\n","        batch_labels = labels[i:i+batch_size]\n","\n","        # Tokenize the text batch\n","        tokenized_batch = tokenizer(batch_texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n","\n","        # Save tokenized texts with corresponding labels\n","        batch_data = {\n","            'input_ids': tokenized_batch['input_ids'],\n","            'attention_mask': tokenized_batch['attention_mask'],\n","            'labels': torch.tensor(batch_labels, dtype=torch.long)\n","        }\n","\n","        # Save the batch\n","        torch.save(batch_data, f\"{project_path}/Dataset/processed_{file_prefix}_data/{file_prefix}_batch_{(i//batch_size) + 1}.pt\")\n","\n","    print(f'Finished processing {file_prefix} data')\n","\n","# Tokenize and save train, validation, and test datasets with labels\n","batch_tokenize_and_save_with_labels(train_texts.tolist(), train_labels.tolist(), file_prefix=\"train\")\n","batch_tokenize_and_save_with_labels(val_texts.tolist(), val_labels.tolist(), file_prefix=\"validation\")\n","batch_tokenize_and_save_with_labels(test_texts.tolist(), test_labels.tolist(), file_prefix=\"test\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyO/wFIoIHn8LOlo1mD5vF2A"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}